\chapter{Introduction}
\label{ch:introduction}
\section{Motivation}
what deep nets offer the communications world, why generalized training is needed to make deep nets perform well

\section{State of the Art}
current forms of communications generalized training

discuss rml

\section{Current Issues}
issues in generalized training...what happens when false assumptions made? perturbations?	

\section{Thesis Contributions}
This work contains the following contributions:
\begin{itemize}
\item A low decay, low bias framework is presented in Chapter~\ref{chapter3}, as well as example wave-forms and data sets that have low entropy.
\item Ongoing work on wireless channel domain adaptation in Chapter~\ref{chapter4}, including neural net architecture and initial results.
\end{itemize}

\section{Thesis Organization}
The thesis will be organized as follows: Chapter~\ref{chapter2} will give a survey of background knowledge learned by the author on the topics of wireless channel modeling (Section~\ref{chanmods}), neural networks with an emphasis on training and data sets (Section~\ref{training}), and modulation classification (Section~\ref{modclass}). Chapter~\ref{chapter3} presents the author's work on generalized training through the development and use of a low bias, low decay framework that synthesizes low-entropy data sets modeling state-of-the-art wave-forms. Finally, Chapter~\ref{chapter4} present's the author's ongoing work on generalized training through the use of the domain adaptation technique, and concluding thoughts are discussed in Chapter~\ref{conclusion}.

\section{List of Related Publications}
The following publications resulted from the activities of this thesis research:
\begin{itemize}
	\item K. McClintick, A. Wyglinski. “Physical Layer Neural Network Framework for Training Data Formation.” VTC Chicago, Fall 2018.
	\item K. Gill, K. McClintick, N. Kanthasamy, “Experimental Test-Bed for Bumblebee-Inspired Channel Selection in an Ad-hoc Network.” VTC Chicago, Fall 2018.
\end{itemize}