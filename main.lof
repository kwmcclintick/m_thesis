\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A flow chart of a transmit and receive chain of communications tasks. Arrows indicate movement of information from one block to another. The form that information takes at each step is communicated through annotations. Antennas are pictured as upside-down triangles.\relax }}{4}{figure.caption.4}
\contentsline {figure}{\numberline {2.2}{\ignorespaces A cartoon showing the sum of several Gaussian PDFs (\ref {eq:gaussianpdf}) at varying frequencies. Amplitude and location of PDFs are arbitrary and not representative of any measurement or formal model.\relax }}{6}{figure.caption.5}
\contentsline {figure}{\numberline {2.3}{\ignorespaces The discrete delay channel model. Inputs each have isolated time delays $\tau _i$, ray powers $|\beta _i|^2=A_0 a_i/d_i$, and ray phases $e^{j\phi _i}$.\relax }}{7}{figure.caption.6}
\contentsline {figure}{\numberline {2.4}{\ignorespaces A series of narrow-band transmissions from\nobreakspace {}\cite {pahlavan2005wireless}, received in a room obtained by the 2D ray-tracing model (\ref {eq:prnarrow}). a) Line of Sight (LOS) path (no reflections). b) First-order reflection ($K=2$ for coefficients equation (\ref {eq:coeff})). c) Second-order reflection, $K=3$. d) Third-order reflection, $K=4$. Notice that higher order reflections have higher frequency changes in power received as distance increases, and that the average power (black line) decreases with distance due to (\ref {eq:friis}).\relax }}{8}{figure.caption.8}
\contentsline {figure}{\numberline {2.5}{\ignorespaces A series of wide-band transmissions from\nobreakspace {}\cite {pahlavan2005wireless}, received in a room obtained by the 2D ray-tracing model (\ref {eq:prnarrow}). a) Line of Sight (LOS) path (no reflections). b) First-order reflection ($K=2$ for coefficients equation (\ref {eq:coeff})). c) Second-order reflection, $K=3$. d) Third-order reflection, $K=4$. Average power (black line) decreases with distance due to (\ref {eq:friis}). For higher order reflections the power received is higher because the impulse signals are not totally isolated.\relax }}{10}{figure.caption.10}
\contentsline {figure}{\numberline {2.6}{\ignorespaces An illusration\nobreakspace {}\cite {rappaport1996wireless} of (\ref {eq:diffraction}), where $\Upsilon $ is the transmitter, $R$ is the receiver, $h$ is the height of the obstruction starting from the direct path from $\Upsilon $ to $R$, and $d_1, d_2$ from the transmitter and receiver to the obstruction, respectively. The Huygens secondary source mimics a potentially strong reflected path, often taking the form of a reflection off a layer of the earth's ionosphere (see Section\nobreakspace {}\ref {terrestrial}).\relax }}{12}{figure.caption.12}
\contentsline {figure}{\numberline {2.7}{\ignorespaces A plot\nobreakspace {}\cite {rappaport1996wireless} of correction factor, $G_{AREA}$ to be used in (\ref {eq:okuhata}) for various frequencies in open, quasi-open, and suburban areas (urban not listed).\relax }}{13}{figure.caption.14}
\contentsline {figure}{\numberline {2.8}{\ignorespaces A plot\nobreakspace {}\cite {rappaport1996wireless} of $A_{mu}(f,d)$, the median attenuation relative to free space, used in (\ref {eq:okuhata}). Plot assumes base-station height $h_t=200 m$, mobile receiver height $h_r=3 m$, an Urban area, and various distances and frequencies.\relax }}{14}{figure.caption.15}
\contentsline {figure}{\numberline {2.9}{\ignorespaces An illustration\nobreakspace {}\cite {pahlavan2005wireless} of a radio link between a moving transmitter (left) and stationary receiver (right). Transmitter is moving at velocity $V_m$ at an instantaneous distance $d_0$ from the receiver.\relax }}{15}{figure.caption.17}
\contentsline {figure}{\numberline {2.10}{\ignorespaces A flow chart\nobreakspace {}\cite {pahlavan2005wireless} summarizing equations (\ref {eq:Rhh}) through (\ref {eq:RHH}). Arrows indicate Fourier (down) and inverse Fourier (up) transforms.\relax }}{17}{figure.caption.18}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Jakes Doppler spectrum\nobreakspace {}\cite {pahlavan2005wireless} for a non-line-of-sight (a) and line-of-sight (b) transmission. Horizontal axis is normalized frequency, and frequency offset is maximal at $\pm f_M$, or movement directly away from and towards the receiver. The impulse in (b) indicates the Doppler shift associated with the line-of-sight ray.\relax }}{18}{figure.caption.19}
\contentsline {figure}{\numberline {2.12}{\ignorespaces A series of impulse responses (\ref {eq:multipathimpulse}) and their Fourier transforms\nobreakspace {}\cite {pahlavan2005wireless}. (a) shows a LOS experiment using a stationary radio transmit-receive pair in a stationary environment ($B_D=0 Hz$ see (\ref {eq:doppspread})). (b) display results of LOS experiment using a stationary receiver, but mobile transmitter that moved randomly within a 12 meter radius of a fixed point, simulating a mobile user pacing on their telephone ($B_D=4.9 Hz$). (c) display results of an obstructed LOS (OLOS) experiment using stationary devices 4 meters apart, but with heavy pedestrian traffic around the transmitter ($B_D=5.7 Hz$). (d) display an OLOS experiment with stationary devices, but the transmitter is rotated at a rate of 2.5 rotations per second ($B_D=5.2 Hz$).\relax }}{19}{figure.caption.20}
\contentsline {figure}{\numberline {2.13}{\ignorespaces A plot\nobreakspace {}\cite {cmbr} of the intensity of the CMBR over frequency.\relax }}{21}{figure.caption.27}
\contentsline {figure}{\numberline {2.14}{\ignorespaces An illustration\nobreakspace {}\cite {rappaport1996wireless} of the base-band signal $m(t)$ being up-converted to the intermediate frequency $f_c$ by a mixer, where the carrier wave-form $A_c cos(2\pi f_c t)$ is generated by a local oscillator, and in this case half of the signal's bandwidth is filtered out (see Figure\nobreakspace {}\ref {fig:upconv} for a double side-band plots of the base-band spectrum of $m(t)$ and the up-converted spectrum of a $S_{DSB}(t)$.\relax }}{23}{figure.caption.29}
\contentsline {figure}{\numberline {2.15}{\ignorespaces A plot\nobreakspace {}\cite {rappaport1996wireless} of (a) the base-band magnitude spectrum $|M(f)|$ of $m(t)$ (see Figure\nobreakspace {}\ref {fig:mixer}), and (b) its up-converted double side-band magnitude spectrum $|S_{AM}(f)|$.\relax }}{23}{figure.caption.30}
\contentsline {figure}{\numberline {2.16}{\ignorespaces \relax }}{24}{figure.caption.33}
\contentsline {figure}{\numberline {2.17}{\ignorespaces \relax }}{25}{figure.caption.34}
\contentsline {figure}{\numberline {2.18}{\ignorespaces \relax }}{25}{figure.caption.35}
\contentsline {figure}{\numberline {2.19}{\ignorespaces \relax }}{25}{figure.caption.36}
\contentsline {figure}{\numberline {2.20}{\ignorespaces An illustration\nobreakspace {}\cite {rappaport1996wireless} of the in-phase and quadrature paths of the modulator block in a RFFE (see Figure\nobreakspace {}\ref {fig:fulltxrx}). The signals $\mathaccentV {dot}05F{m}(t), m(t)$ may not experience the same gains or appropriate phases of 0 and 90 degrees (\ref {eq:iqimbal}).\relax }}{27}{figure.caption.38}
\contentsline {figure}{\numberline {2.21}{\ignorespaces Three plots\nobreakspace {}\cite {quant} describing an example of the effects of quantization error on a section of a ramp signal, (left) sine wave, (middle) and noisy wave-form (right). These plots have a digital resolution of one, all wave-forms can only be represented by a zero value, one, two, or three.\relax }}{28}{figure.caption.40}
\contentsline {figure}{\numberline {2.22}{\ignorespaces A section of a time-domain square wave-form formulated by summing cosines. While the states of the square wave-form aims to have values of negative and positive one, there are large deviations near high-definition edges, and small ripples in flat sections. If unlucky, these analog deviations can sum to mV values.\relax }}{31}{figure.caption.42}
\contentsline {figure}{\numberline {2.23}{\ignorespaces An illustration\nobreakspace {}\cite {cs231} of a biological neuron. A neuron cell is composed of a nucleus which receives signals from many dendrites. The amount of influence a dendrite has on a neuron is determined by synapses. When the sum of incoming signals is above a threshold, the nucleus fires a signal down its axon, which in turn splits into many dendrites, feeding into other neurons.\relax }}{33}{figure.caption.44}
\contentsline {figure}{\numberline {2.24}{\ignorespaces A mathematical representation\nobreakspace {}\cite {cs231} of Figure\nobreakspace {}\ref {fig:sub:cell}. The previous neurons' axons carry the signals $x_0, x_1, x_2$, which split into many dendrites. Synapses influence that value by a weight, ($w_0, w_1, w_2$). The cell body adds weights to each incoming dendrite ($b_0, b_1, b_2$), computes the dot product of all dendrites, and outputs a signal on its axon defined as the output of some activation function $f$ whose input is the dot product.\relax }}{33}{figure.caption.45}
\contentsline {figure}{\numberline {2.25}{\ignorespaces An illustration\nobreakspace {}\cite {cs231} of reducing $W \in [3,4 ]$ and $b \in [3,1 ]$ into a single matrix, $W \in [3,5 ]$ by adding a unit value to the end of $x_i \in [5,1 ]$.\relax }}{34}{figure.caption.47}
\contentsline {figure}{\numberline {2.26}{\ignorespaces An illustration\nobreakspace {}\cite {cs231} of the computation of class cores $f$ and the resulting loss score $L_i$ using both SVM and soft-max functions. Both use the same class scores $f$, but have very different interpretations of their results, 1.58 and 1.04. The SVM considers each incorrect score less than a margin below the correct score as a contributor to loss, while the soft-max classifier relays a value proportional to the belief that the label assigned to each signal is correct.\relax }}{37}{figure.caption.49}
\contentsline {figure}{\numberline {2.27}{\ignorespaces An illustration\nobreakspace {}\cite {cs231} of common data splits between training, validation, and testing data. In this image, validation is performed on fold 5, training on folds 1-4, and testing on the rest. Next, fold 1 would be used as validation data, and folds 2-5 as training data, and so on, until all 5 folds have been used as the validation data.\relax }}{39}{figure.caption.51}
\contentsline {figure}{\numberline {2.28}{\ignorespaces An example SVM loss function\nobreakspace {}\cite {cs231} plotted against two weights. Red represents high loss, blue low loss. Each of the two axis represent values assigned to a weight. In practice, loss functions have pockets of local minima/maxima, and cannot be visualized due to the number of dimensions required to represent each weight used.\relax }}{40}{figure.caption.53}
\contentsline {figure}{\numberline {2.29}{\ignorespaces An illustration\nobreakspace {}\cite {cs231} of an SVM loss function's gradient vector (\ref {eq:gradient}) for a two-weight linear classifier. Red represents high loss, blue low loss. The white circle represents the current values chosen for weights $w_o, w_1$, the white arrow the gradients unit vector, and the dashed line an extension of that gradient. Updating the weights by too much will put the weights in perhaps a higher loss section of the graph, but updating by too little will be computationally expensive and perhaps get the SGD stuck in a local minimum of the SVM loss function.\relax }}{41}{figure.caption.54}
\contentsline {figure}{\numberline {2.30}{\ignorespaces The in-phase components of one positive and one negative QPSK symbol, up-sampled to 16 SPS by a Raised-Root Cosine (RRC) filter with a roll-off coefficient of 0.35. Making up half the values of an example flattened training signal vector $x_i$, classification decisions of a linear classifier using this signal would likely depend most heavily on samples surrounding the 60th and 80th sample, as they most strongly correlate to what bits are being transmitted. As a result, weights corresponding to those samples would likely be pushed to higher values during SGD.\relax }}{42}{figure.caption.55}
\contentsline {figure}{\numberline {2.31}{\ignorespaces A circuit model\nobreakspace {}\cite {cs231} showing the forward pass (green) by applying inputs to the gates operators and backward pass (red) by applying the chain rule recursively. Gates represent a few local operations done by a linear classifier's neurons. Gates can do both passes totally independent of other gates, without knowledge of the full circuit, or classifier structure.\relax }}{45}{figure.caption.58}
\contentsline {figure}{\numberline {2.32}{\ignorespaces A circuit model\nobreakspace {}\cite {cs231} showing the forward pass (green) by applying inputs to the gates operators and backward pass (red) by applying the chain rule recursively of a circuit featuring a ReLU max gate. The blacked out $w$ weight would cause all gates before it to have a gradient of zero, killing those neurons. Using (\ref {eq:gradient}), the back pass value for $w$ can be shown to be $w = 1 \times \frac {\delta }{\delta a}2a \times \frac {\delta }{\delta r}(r+p) \times \frac {\delta }{\delta w}\qopname \relax m{max}(w,z) = 1 \times 2 \times 1 \times 0 = 0$\relax }}{48}{figure.caption.61}
\contentsline {figure}{\numberline {2.33}{\ignorespaces A three-layer\nobreakspace {}\cite {cs231} neural network with two fully-connected hidden layers. Each hidden layer has four neurons, and the input has three samples. As shown in Section\nobreakspace {}\ref {cnn}, not all architectures use fully connected layers, and for good reason.\relax }}{51}{figure.caption.64}
\contentsline {figure}{\numberline {2.34}{\ignorespaces A diagram from\nobreakspace {}\cite {universalapproxarticle}. Focusing on the top neuron, the output (shown) is computed as the sigmoid (\ref {eq:sig}) of the dot product (see Figure\nobreakspace {}\ref {fig:sub:neuron}) of all its inputs, in this case the one input. $w$ shapes the transient part of the output, while $b$ places it. The function saturates at zero and one.\relax }}{52}{figure.caption.66}
\contentsline {figure}{\numberline {2.35}{\ignorespaces A diagram from\nobreakspace {}\cite {universalapproxarticle}. With each additional neuron in the hidden layer, the sum of sigmoids at the neuron in the subsequent layer is a closer approximation of our cartoon of a complex function.\relax }}{53}{figure.caption.67}
\contentsline {figure}{\numberline {2.36}{\ignorespaces A comparable CNN\nobreakspace {}\cite {cs231} to the linear classifier in Figure\nobreakspace {}\ref {fig:threelayernn}. Convolutional layers are three-dimensional, and only the last few layers are fully connected. The rest of the CNN is much more sparsely connected in an effort to reduce over-fitting and computational cost.\relax }}{54}{figure.caption.68}
\contentsline {figure}{\numberline {2.37}{\ignorespaces Two convolution computations\nobreakspace {}\cite {cs231} applied to an input (blue) of size $W=5$, filters (red) of size $F=3$, zero-padding (gray) $P=1$, and stride $S=2$. Through (\ref {eq:outsize}), we obtain the output matrix (green) height/width $(5-3+2\times 1)/2+1=3$ of depth two due to using two filters for an output shape $\in [3,3,2]$. The value $3$ is computed as the sum of all highlighted convolutions $x\circledast w0$, plus the bias $b0$.\relax }}{56}{figure.caption.70}
\contentsline {figure}{\numberline {2.38}{\ignorespaces Max pooling\nobreakspace {}\cite {cs231} of a 244 by 244 pixel image. Input size $W$ is 224, filter size $F$ is two, stride $S$ is two for an output shape (\ref {eq:outsize}) of $(224-2+2\times 0)/2+1=112$. Depth is maintained.\relax }}{57}{figure.caption.71}
\contentsline {figure}{\numberline {2.39}{\ignorespaces A ConvNet architecture\nobreakspace {}\cite {cs231} that takes raw image pixel values as input, and outputs a five-element fully-connected layer, where each value corresponds to the CNN's belief that the raw image belongs to a label. In this case, the image is likely a car.\relax }}{58}{figure.caption.72}
\contentsline {figure}{\numberline {2.40}{\ignorespaces An illustration\nobreakspace {}\cite {cs231} of the last few layers of a linear classifier before (a) and after (b) dropout layers are implemented. Arrows represent connections between neurons, while neurons with x's through them represent neuron connections terminated by being dropped out.\relax }}{60}{figure.caption.74}
\contentsline {figure}{\numberline {2.41}{\ignorespaces An illustration\nobreakspace {}\cite {bayesian} of three time iterations of (\ref {eq:bayesianfun}). The black line is the estimated objective or loss function $f$, while the dashed black line is the true $f$ (unknown but visualized). The acquisition function $\alpha $ is in green, whose maxima are highlighted with red arrows, indicating either exploration (when uncertainty $\sigma (\cdot )$, blue, is large) or exploitation (model prediction is high, solid and dashed black lines match). Observations $x_n$ are marked as black dots, with the new observations in the n=3 and n=4 sub-figures highlighted in red. Notice how new observations reduce uncertainty, and are first taken at high value points (right skewed) to maximize impact on acquisition function reduction.\relax }}{61}{figure.caption.75}
\contentsline {figure}{\numberline {2.42}{\ignorespaces A series of decoy images\nobreakspace {}\cite {fooled} fooling a computer vision classifier. Most images fail to remotely resemble their target label, however due to the brittle nature of training neural networks with standard back-pass techniques, small movements in feature-space at evaluation time can have significant and catastrophic results.\relax }}{66}{figure.caption.78}
\contentsline {figure}{\numberline {2.43}{\ignorespaces A flow diagram\nobreakspace {}\cite {gan} of a GAN process. Synthetic data samples are added to data samples observed by the discriminator.\relax }}{67}{figure.caption.79}
\contentsline {figure}{\numberline {2.44}{\ignorespaces A flow diagram\nobreakspace {}\cite {gan} describing the SGD (\ref {eq:gradient}) feedback loop between the generator and discriminator (see Figure\nobreakspace {}\ref {fig:gan}). Parameter updates continue until the learning capacity of the networks are reached and classification accuracy of the discriminator reaches a steady state value $\in (0.5, 1)$.\relax }}{67}{figure.caption.80}
\contentsline {figure}{\numberline {2.45}{\ignorespaces A series of testing images and classification results\nobreakspace {}\cite {dadaptHRL}. A) Test image collected from real Cityscapes dataset. B) Identity mapped version of the image. C) Image translated to the target domain. D) Evaluation of translated image without domain adaptation. E) Evaluation of translated image using domain adaptation\nobreakspace {}\cite {dadaptHRL}. F) Translated image ground truth.\relax }}{69}{figure.caption.81}
\contentsline {figure}{\numberline {2.46}{\ignorespaces A flow diagram\nobreakspace {}\cite {dadaptHRL} describing the various transforms $f_x, g_x, h, f_y, g_y$ and spaces $X$, $Z$, $Y$, $C$ and their interactions at the highest level in domain adaptation. The field is motivated by scarcity of annotated real pictures, but has much wider applications. Implemented correctly, training of classifiers becomes highly generalizable, making testing well under conditions not trained under becomes very robust when domain adaptation is performed on a set of unlabeled data from the new target domain.\relax }}{70}{figure.caption.82}
\contentsline {figure}{\numberline {2.47}{\ignorespaces An elaborated\nobreakspace {}\cite {dadaptHRL} flow diagram of Figure\nobreakspace {}\ref {fig:adaptflow}, describing additionally the various weighted loss functions $Q_c, Q_{id}, Q_z, Q_{tr}, Q_{cyc}, Q_{trc}$ and how they interact with each domain $X$, $Y$, and $Z$. See equations (\ref {eq:qc}) through (\ref {eq:qtrc}).\relax }}{72}{figure.caption.83}
\contentsline {figure}{\numberline {2.48}{\ignorespaces A very high-level flow diagram\nobreakspace {}\cite {mods} describing the flow of information in a communications transmit-receive pair. Information begins as bits mapped to IQ points, is transformed into a voltage by a DAC, transduced into an electromagnetic wave by a transmitting antenna, travels through a noisy channel, is transduced back into a voltage by a receiving antenna, detected and transformed into IQ points with the help of a ADC, and finally mapped back to bits.\relax }}{73}{figure.caption.84}
\contentsline {figure}{\numberline {2.49}{\ignorespaces A set of QPSK constellation points (\ref {eq:qpsk}) for $E_s=4$. The horizontal axis is defined as $\phi _1(t)$ or the real valued element in a complex tuple, and the vertical axis as $\phi _2(t)$, traditionally represented as the imaginary valued element in a complex tuple. The resulting transformations are $n = 1 : b\rightarrow (0, 0) : s \rightarrow (2/\sqrt {2}, 2/\sqrt {2})$, $n = 2 : b\rightarrow (0, 1) : s \rightarrow (-2/\sqrt {2}, 2/\sqrt {2})$, $n = 3 : b\rightarrow (1, 0) : s \rightarrow (-2/\sqrt {2}, -2/\sqrt {2})$, and $n = 4 : b\rightarrow (1, 1) : s \rightarrow (2/\sqrt {2}, -2/\sqrt {2})$\relax }}{75}{figure.caption.86}
\contentsline {figure}{\numberline {2.50}{\ignorespaces A flow chart\nobreakspace {}\cite {modclassback} describing the forward pass (see Figure\nobreakspace {}\ref {fig:relucircuit}) of a set of eight input values through the CLDNN. A $[1,8]$ input vector is concatenated with values filtered through a $[1,8]$ filter in both the first and second convolutional layer. Each filter (see Figure\nobreakspace {}\ref {fig:filtresp}) contains eight weights and one bias value (see Figure\nobreakspace {}\ref {fig:filter} for example filters), which are calculated during SGD (\ref {eq:gradient}). The Long Short-Term Memory (LSTM) cell holds the values for the soft-max classification layer.\relax }}{76}{figure.caption.87}
\contentsline {figure}{\numberline {2.51}{\ignorespaces The time-domain IQ plot\nobreakspace {}\cite {modclassback} of a $[2,128]$ output signal from the $[1,8]$ filter in Figure\nobreakspace {}\ref {fig:filtresp}. The signal input to the filter was random, but trained to maximally activate the filters eight weights. The result is a Binary Phase Shift Keying (BPSK) waveform, indicating that this filter was trained to maximize the eventual soft-max class scores of BPSK signals.\relax }}{77}{figure.caption.88}
\contentsline {figure}{\numberline {2.52}{\ignorespaces Time (top) and frequency (bottom) domain magnitude plots\nobreakspace {}\cite {modclassback} of a trained $[1,8]$ filter like those in Figure\nobreakspace {}\ref {fig:cldnn}. This filter's first, second, and seventh weights have the most influence on classification. See Figure\nobreakspace {}\ref {fig:trainsig} for another visualization of this filter.\relax }}{77}{figure.caption.89}
\contentsline {figure}{\numberline {2.53}{\ignorespaces A confusion matrix\nobreakspace {}\cite {modclassback} describing the classification accuracy of the CLDNN on all test signals at evaluation time. The color gradient communicates classification accuracy averaged over SNR values ranging from -20 dB to 20dB. The horizontal axis displays the modulation scheme that the CLDNN classifies signals by, and the vertical axis the ground truth of those signals. A perfectly performing classifier would have a deep brown diagonal matrix, where each signal of each modulation type of each SNR is correctly classified by having the highest soft-max value at its index corresponding to the signals' ground truth label.\relax }}{78}{figure.caption.90}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Illustration of the proposed framework and the ChannelPush.py script. SampBasic.hdf5 acts as the Dataset Under Test (DUT) while ChannelConfig.ini as the instructions file. SampOut.hdf5 files are written as outputs. The 3D matrix is formed by the instructions file, containing the 2D matrix's (see Table \ref {table:tab3}) instance variables. The 2D matrix objects are formed by run-time channel class imports. 1D channel sequences (see Figure \ref {fig:1dlist}) are formed by permuting the channel imperfection objects from the 2D matrix, and the DUT is pushed sample by sample through each sequence in parallel.\relax }}{81}{figure.caption.91}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Example set of eight 1D channel sequences (refer to Figure\nobreakspace {}\ref {fig:channeltoolsub}) formed by permuting through the 2D channel object matrix. SampBasic.hdf5 is the DUT, and is pushed through each sequence sample by sample, leveraging Multiprocessing.\relax }}{85}{figure.caption.93}
\contentsline {figure}{\numberline {3.3}{\ignorespaces 1 SPS pulse shaped Quadrature Phase-Shift Keying (QPSK) IQ data representing the baseband data of an Ettus Research N210 transmission. For the sake of visualization, frequency offset from Local Oscillator (LO) drift has been left out. The top track displays the dataset influenced by phase ambiguity and AWGN, then the matched filtering of that data. The bottom track additionally shows STO, where the data is interpolated and filtered up to an intermediate 2 SPS, offset in time, then decimated (and once again match filtered like the top track).\relax }}{87}{figure.caption.95}
\contentsline {figure}{\numberline {3.4}{\ignorespaces The AWGN channel effect is described by its SNR and Gaussian RV variance, $\sigma $. Three AWGN channels of varying SNR but constant $\sigma $ described by ChannelConfig.ini are applied to the same infile sampBasicmod.py. The outputs of which are manually moved to Intermediate Frequency (IF) folders corresponding to a secondary instructions file, MergeConfig.ini. Merge\_datasets.py (see Figure\nobreakspace {}\ref {fig:merge/}) modulates and sums the independent transmissions.\relax }}{88}{figure.caption.96}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Three 16 SPS pulse shaped QPSK datasets from Figure\nobreakspace {}\ref {fig:bashcmd_poster/} are modulated to intermediate frequencies 10, 15, and 20 MHz. Each dataset was pushed through the framework as a DUT and modified by a unique AWGN channel block independently, each representing a transmitted signal. Future work will implement this feature to produce MIMO and OFDM datasets.\relax }}{89}{figure.caption.97}
\contentsline {figure}{\numberline {3.6}{\ignorespaces RML2016.10A is composed of 1,000 training sets containing 128 samples each per class per SNR value. Transmissions average 28.3 bits divergence from theory. The proposed application (see Figure\nobreakspace {}\ref {fig:1spsslides}) averages 36.2 bit divergence from theory. In order to achieve similar KLD entropy at an RF NNs evaluation time to state-of-the-art datasets, this analysis shows our proposed application requires at least 256 samples per transmission. The resulting divergence from theory is 25 bits, or a 11.58\% decrease from RML2016.10A.\relax }}{91}{figure.caption.98}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
